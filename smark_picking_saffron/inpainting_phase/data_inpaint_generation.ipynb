{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2068ecad",
   "metadata": {},
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# Base directories\n",
    "dataset_root = 'open_close'\n",
    "output_root = 'dataset_masked'\n",
    "splits = ['train', 'valid']\n",
    "\n",
    "def create_mask(image_shape, polygons):\n",
    "    \"\"\"Creates a binary mask from list of polygons.\"\"\"\n",
    "    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "    for pts in polygons:\n",
    "        # Convert to np.array of shape (n, 1, 2) and type int32\n",
    "        pts_array = np.array(pts, dtype=np.int32).reshape((-1, 1, 2))\n",
    "        # Fill the polygon in white (255)\n",
    "        cv2.fillPoly(mask, [pts_array], color=255)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def parse_annotation(txt_path, width, height):\n",
    "    \"\"\"Parses label .txt file with: class_id x1 y1 x2 y2 ...\"\"\"\n",
    "    polygons = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 7 or (len(parts) - 1) % 2 != 0:\n",
    "                print(f\"⚠️  Invalid polygon format at {txt_path}, line {line_num}\")\n",
    "                continue\n",
    "\n",
    "            # Skip the class ID\n",
    "            coords = parts[1:]\n",
    "\n",
    "            try:\n",
    "                # Group into (x, y) pairs\n",
    "                pts = []\n",
    "                for i in range(0, len(coords), 2):\n",
    "                    x = float(coords[i])\n",
    "                    y = float(coords[i+1])\n",
    "                    if not (0 <= x <= 1 and 0 <= y <= 1):\n",
    "                        print(f\"⚠️  Normalized point out of range: ({x}, {y}) in {txt_path}\")\n",
    "                        continue\n",
    "                    px = int(x * width)\n",
    "                    py = int(y * height)\n",
    "                    pts.append((px, py))\n",
    "                if len(pts) >= 3:\n",
    "                    polygons.append(pts)\n",
    "            except ValueError:\n",
    "                print(f\"⚠️  Non-numeric values in {txt_path}, line {line_num}\")\n",
    "    return polygons\n",
    "\n",
    "\n",
    "def process_dataset():\n",
    "    for split in splits:\n",
    "        print(f\"Processing split: {split}\")\n",
    "        img_dir = os.path.join(dataset_root, split, 'images')\n",
    "        label_dir = os.path.join(dataset_root, split, 'labels')\n",
    "\n",
    "        new_img_dir = os.path.join(output_root, split, 'images')\n",
    "        new_label_dir = os.path.join(output_root, split, 'labels')\n",
    "        os.makedirs(new_img_dir, exist_ok=True)\n",
    "        os.makedirs(new_label_dir, exist_ok=True)\n",
    "\n",
    "        for img_path in glob(os.path.join(img_dir, '*')):\n",
    "            img_name = os.path.basename(img_path)\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            txt_path = os.path.join(label_dir, base_name + '.txt')\n",
    "\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            if not os.path.exists(txt_path):\n",
    "                print(f\"Warning: Label file not found for {img_name}\")\n",
    "                continue\n",
    "\n",
    "            polygons = parse_annotation(txt_path, width, height)\n",
    "            mask = create_mask(image.shape, polygons)\n",
    "\n",
    "            # Save original image (copy)\n",
    "            cv2.imwrite(os.path.join(new_img_dir, img_name), image)\n",
    "            # Save mask\n",
    "            cv2.imwrite(os.path.join(new_label_dir, base_name + '.jpg'), mask)\n",
    "\n",
    "    print(f\"\\n✅ Conversion complete. New dataset saved at: {output_root}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5515ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n",
      "Processing split: valid\n",
      "\n",
      "✅ Conversion complete. Metadata saved at: dataset_masked/metadata/summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import csv\n",
    "from glob import glob\n",
    "\n",
    "# Base directories\n",
    "dataset_root = 'open_close'\n",
    "output_root = 'dataset_masked'\n",
    "splits = ['train', 'valid']\n",
    "\n",
    "metadata_dir = os.path.join(output_root, 'metadata')\n",
    "os.makedirs(metadata_dir, exist_ok=True)\n",
    "csv_path = os.path.join(metadata_dir, 'summary.csv')\n",
    "\n",
    "def create_mask(image_shape, polygons):\n",
    "    mask = np.zeros(image_shape[:2], dtype=np.uint8)\n",
    "    for pts in polygons:\n",
    "        pts_array = np.array(pts, dtype=np.int32).reshape((-1, 1, 2))\n",
    "        cv2.fillPoly(mask, [pts_array], color=255)\n",
    "    return mask\n",
    "\n",
    "def parse_annotation(txt_path, width, height):\n",
    "    polygons = []\n",
    "    class_counts = {1: 0, 2: 0}\n",
    "    with open(txt_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 7 or (len(parts) - 1) % 2 != 0:\n",
    "                print(f\"⚠️  Invalid polygon format at {txt_path}, line {line_num}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                class_id = int(parts[0])\n",
    "                coords = parts[1:]\n",
    "                pts = []\n",
    "                for i in range(0, len(coords), 2):\n",
    "                    x = float(coords[i])\n",
    "                    y = float(coords[i+1])\n",
    "                    if not (0 <= x <= 1 and 0 <= y <= 1):\n",
    "                        print(f\"⚠️  Normalized point out of range: ({x}, {y}) in {txt_path}\")\n",
    "                        continue\n",
    "                    px = int(x * width)\n",
    "                    py = int(y * height)\n",
    "                    pts.append((px, py))\n",
    "                if len(pts) >= 3:\n",
    "                    polygons.append(pts)\n",
    "                    if class_id in class_counts:\n",
    "                        class_counts[class_id] += 1\n",
    "            except ValueError:\n",
    "                print(f\"⚠️  Non-numeric values in {txt_path}, line {line_num}\")\n",
    "    return polygons, class_counts\n",
    "\n",
    "def resize_to_256(image_or_mask):\n",
    "    return cv2.resize(image_or_mask, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def process_dataset():\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['image_path', 'description'])\n",
    "\n",
    "        for split in splits:\n",
    "            print(f\"Processing split: {split}\")\n",
    "            img_dir = os.path.join(dataset_root, split, 'images')\n",
    "            label_dir = os.path.join(dataset_root, split, 'labels')\n",
    "\n",
    "            new_img_dir = os.path.join(output_root, split, 'images')\n",
    "            new_label_dir = os.path.join(output_root, split, 'labels')\n",
    "            os.makedirs(new_img_dir, exist_ok=True)\n",
    "            os.makedirs(new_label_dir, exist_ok=True)\n",
    "\n",
    "            for img_path in glob(os.path.join(img_dir, '*')):\n",
    "                img_name = os.path.basename(img_path)\n",
    "                base_name = os.path.splitext(img_name)[0]\n",
    "                txt_path = os.path.join(label_dir, base_name + '.txt')\n",
    "\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Could not read image: {img_path}\")\n",
    "                    continue\n",
    "\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                if not os.path.exists(txt_path):\n",
    "                    print(f\"Warning: Label file not found for {img_name}\")\n",
    "                    continue\n",
    "\n",
    "                polygons, class_counts = parse_annotation(txt_path, width, height)\n",
    "                mask = create_mask(image.shape, polygons)\n",
    "\n",
    "                # Resize\n",
    "                resized_image = resize_to_256(image)\n",
    "                resized_mask = resize_to_256(mask)\n",
    "\n",
    "                # Save\n",
    "                save_img_path = os.path.join(new_img_dir, img_name)\n",
    "                save_mask_path = os.path.join(new_label_dir, base_name + '.jpg')\n",
    "                cv2.imwrite(save_img_path, resized_image)\n",
    "                cv2.imwrite(save_mask_path, resized_mask)\n",
    "\n",
    "                # Write to CSV\n",
    "                abs_img_path = os.path.abspath(save_img_path)\n",
    "                desc = f\"There are {class_counts.get(2, 0)} open-saffron flower and {class_counts.get(1, 0)} close-saffron flower\"\n",
    "                writer.writerow([abs_img_path, desc])\n",
    "\n",
    "    print(f\"\\n✅ Conversion complete. Metadata saved at: {csv_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7227efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train paths...\n",
      "Writing val paths...\n",
      "Writing test paths...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2  # <- Required for grayscale conversion\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "# Original dataset paths\n",
    "train_img_dir = 'dataset_masked/train/images'\n",
    "val_img_dir = 'dataset_masked/valid/images'\n",
    "\n",
    "# Mask dirs (output)\n",
    "train_mask_dir = 'dataset_masked/train/labels'\n",
    "val_mask_dir = 'dataset_masked/valid/labels'\n",
    "\n",
    "# Output txt files\n",
    "data_path = './dataset_masked/txt/train_path.txt'\n",
    "mask_path = './dataset_masked/txt/train_mask_path.txt'\n",
    "val_path = './dataset_masked/txt/val_path.txt'\n",
    "val_mask_path = './dataset_masked/txt/val_mask_path.txt'\n",
    "test_path = './dataset_masked/txt/test_path.txt'\n",
    "test_mask_path = './dataset_masked/txt/test_mask_path.txt'\n",
    "\n",
    "os.makedirs('./dataset_masked/txt', exist_ok=True)\n",
    "\n",
    "# 1. Get all train images\n",
    "train_images = sorted([f for f in os.listdir(train_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# 2. Split into train (90%) and val (10%)\n",
    "split_idx = int(len(train_images) * 0.9)\n",
    "train_split = train_images[:split_idx]\n",
    "val_split = train_images[split_idx:]\n",
    "\n",
    "# 3. Get all val images (will be used as test set)\n",
    "test_images = sorted([f for f in os.listdir(val_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "def convert_to_grayscale_if_needed(mask_path):\n",
    "    \"\"\"Ensure the mask is in grayscale and overwrite if not.\"\"\"\n",
    "    mask = cv2.imread(mask_path)\n",
    "    if mask is None:\n",
    "        print(f\"[WARNING] Could not read mask: {mask_path}\")\n",
    "        return\n",
    "    if len(mask.shape) == 3:  # If not grayscale\n",
    "        gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        cv2.imwrite(mask_path, gray_mask)\n",
    "\n",
    "def write_paths(images, img_dir, mask_dir, img_outfile, mask_outfile):\n",
    "    with open(img_outfile, 'w') as f_img, open(mask_outfile, 'w') as f_mask:\n",
    "        for img_name in images:\n",
    "            img_path = os.path.abspath(os.path.join(img_dir, img_name))\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            mask_name = base_name + '.jpg'\n",
    "            mask_path_full = os.path.abspath(os.path.join(mask_dir, mask_name))\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Warning: image not found {img_path}\")\n",
    "                continue\n",
    "            if not os.path.exists(mask_path_full):\n",
    "                print(f\"Warning: mask not found {mask_path_full}\")\n",
    "                continue\n",
    "\n",
    "            # ✅ Convert to grayscale if needed\n",
    "            convert_to_grayscale_if_needed(mask_path_full)\n",
    "\n",
    "            f_img.write(img_path + '\\n')\n",
    "            f_mask.write(mask_path_full + '\\n')\n",
    "\n",
    "print(\"Writing train paths...\")\n",
    "write_paths(train_split, train_img_dir, train_mask_dir, data_path, mask_path)\n",
    "\n",
    "print(\"Writing val paths...\")\n",
    "write_paths(val_split, train_img_dir, train_mask_dir, val_path, val_mask_path)\n",
    "\n",
    "print(\"Writing test paths...\")\n",
    "write_paths(test_images, val_img_dir, val_mask_dir, test_path, test_mask_path)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a14906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a92d09b83d4ff0950ffc3647389247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 689 embeddings with path column. Shape: (689, 384)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Ensure compatibility\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Read input CSV (no header)\n",
    "csv_path = './dataset_masked/metadata/summary.csv'\n",
    "df = pd.read_csv(csv_path, header=None)\n",
    "\n",
    "# Extract path and sentence columns\n",
    "paths = df[0].tolist()        # First column\n",
    "sentences = df[1].tolist()    # Second column\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(sentences, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "# Combine paths + embeddings\n",
    "embedding_df = pd.DataFrame(embeddings)\n",
    "embedding_df.insert(0, 'path', paths)  # Insert path as the first column\n",
    "\n",
    "# Save to new CSV\n",
    "embedding_df.to_csv('./dataset_masked/metadata/sentence_embeddings.csv', index=False)\n",
    "\n",
    "print(f\"✅ Saved {len(embeddings)} embeddings with path column. Shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97dd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b8dedff9",
   "metadata": {},
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "# Original dataset paths\n",
    "train_img_dir = 'dataset_masked/train/images'\n",
    "val_img_dir = 'dataset_masked/valid/images'\n",
    "\n",
    "# Mask dirs (output)\n",
    "train_mask_dir = 'dataset_masked/train/labels'\n",
    "val_mask_dir = 'dataset_masked/valid/labels'\n",
    "\n",
    "# Output txt files (val files removed)\n",
    "data_path = './dataset_masked/txt/train_path.txt'\n",
    "mask_path = './dataset_masked/txt/train_mask_path.txt'\n",
    "test_path = './dataset_masked/txt/test_path.txt'\n",
    "test_mask_path = './dataset_masked/txt/test_mask_path.txt'\n",
    "\n",
    "os.makedirs('./dataset_masked/txt', exist_ok=True)\n",
    "\n",
    "# 1. Get all train images (use 100% for training now)\n",
    "train_images = sorted([f for f in os.listdir(train_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "# 2. Get all test images (from 'valid' folder)\n",
    "test_images = sorted([f for f in os.listdir(val_img_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "\n",
    "def write_paths(images, img_dir, mask_dir, img_outfile, mask_outfile):\n",
    "    with open(img_outfile, 'w') as f_img, open(mask_outfile, 'w') as f_mask:\n",
    "        for img_name in images:\n",
    "            img_path = os.path.abspath(os.path.join(img_dir, img_name))\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            mask_name = base_name + '.jpg'\n",
    "            mask_path = os.path.abspath(os.path.join(mask_dir, mask_name))\n",
    "\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"⚠️  Warning: image not found {img_path}\")\n",
    "                continue\n",
    "            if not os.path.exists(mask_path):\n",
    "                print(f\"⚠️  Warning: mask not found {mask_path}\")\n",
    "                continue\n",
    "\n",
    "            f_img.write(img_path + '\\n')\n",
    "            f_mask.write(mask_path + '\\n')\n",
    "\n",
    "print(\"📂 Writing train paths...\")\n",
    "write_paths(train_images, train_img_dir, train_mask_dir, data_path, mask_path)\n",
    "\n",
    "print(\"📂 Writing test paths...\")\n",
    "write_paths(test_images, val_img_dir, val_mask_dir, test_path, test_mask_path)\n",
    "\n",
    "print(\"✅ Done! Only train and test sets are used.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
